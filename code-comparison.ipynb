{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d915a356-0432-494c-a87e-c876905e794c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ADS Channel Summary Weekly-sql0.sql\n",
      "ADS Channel Summary-sql0.sql\n",
      "ADS vs ESF vs SC Dashboard-sql0.sql\n",
      "Metadata_extract_20241223_1749.xlsx\n",
      "Total number of files : 3\n",
      "                                 Report Combinations  Total files  \\\n",
      "0  ADS Channel Summary Weekly-sql0.sql,ADS Channe...            2   \n",
      "\n",
      "   # of tables across all reports  # of joining keys across all reports  \\\n",
      "0                              18                                     1   \n",
      "\n",
      "   # of common tables  # of common joining keys  Table Matching Percentage  \\\n",
      "0                  12                         1                      66.67   \n",
      "\n",
      "   Joining Keys Matching Percentage  Overall Matching Percentage  \\\n",
      "0                             100.0                        83.33   \n",
      "\n",
      "          Recommendation  \n",
      "0  Reports can be merged  \n",
      "2 files can be consolidated to 1\n",
      "Reports that can be fully merged and removed:\n",
      "set()\n",
      "\n",
      "Reports that can be partially merged:\n",
      "Report: ADS Channel Summary Weekly\n",
      "  Total files: 1\n",
      "  Matched files: 1\n",
      "  Reports matched with: {'ADS Channel Summary'}\n",
      "Report: ADS Channel Summary\n",
      "  Total files: 1\n",
      "  Matched files: 1\n",
      "  Reports matched with: {'ADS Channel Summary Weekly'}\n",
      "\n",
      "Within-report file consolidations (files that can be consolidated):\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import configparser\n",
    "from sql_metadata import Parser\n",
    "from datetime import datetime\n",
    "from mo_sql_parsing import parse\n",
    "from mo_sql_parsing import format\n",
    "from itertools import combinations\n",
    "import logging\n",
    "from collections import defaultdict\n",
    "import xml.etree.ElementTree as ET\n",
    "from typing import List\n",
    "\n",
    "\"\"\"\n",
    "Parser is not working as expected for \n",
    "pivot queries\n",
    "IGNORE NULLS OVER (oracledb)\n",
    "* exclude\n",
    "cross join\n",
    "\"\"\"\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(name)s - Line: %(lineno)d - %(message)s')\n",
    "\n",
    "config_file_path = r'C:\\Users\\DOUNDKARSHUBHAMBALU\\Downloads\\sql_reports_analysis\\config.txt'\n",
    "\n",
    "table_details_excel = r\"C:\\Users\\DOUNDKARSHUBHAMBALU\\Downloads\\sql_reports_analysis\\tables_coe.xlsx\"\n",
    "\n",
    "sheet_names = pd.ExcelFile(table_details_excel).sheet_names\n",
    "\n",
    "dfs = []\n",
    "\n",
    "for sheet in sheet_names:\n",
    "    df = pd.read_excel(table_details_excel, sheet_name=sheet)\n",
    "    df = df[['Name','Context']]\n",
    "    dfs.append(df)\n",
    "\n",
    "lookup = pd.concat(dfs, ignore_index=True)\n",
    "\n",
    "lookup[['loc','db','schema']] = lookup['Context'].str.split(' >> ', expand=True)\n",
    "\n",
    "df_column = pd.DataFrame(columns=['Report_Name', 'Db', 'Schema', 'Table', 'Column'])\n",
    "df_table = pd.DataFrame(columns=['Report_Name', 'Table'])\n",
    "df_join_condition = pd.DataFrame(columns=['Report_Name', 'Union/Intersection/Minus if any',\n",
    "                                          'Joining_Type', 'Joining_Condition',\n",
    "                                          'Joining_Columns_With_Table_Alias_if_any',\n",
    "                                          'Joining_Columns_Having_Proper_Table_Details',\n",
    "                                          'LeftTable_Subquery_if_any',\n",
    "                                          'RightTable_Subquery_if_any'])\n",
    " \n",
    " \n",
    "def getconfig():\n",
    "    \"\"\" to grab input and output file location\"\"\"\n",
    "    config = configparser.ConfigParser()\n",
    "    config.read(config_file_path)\n",
    "    config_dict = dict(config.items('PATH'))\n",
    "    return config_dict\n",
    " \n",
    "def extract_with(sql_dict,c=0):\n",
    "    columns = []\n",
    "\n",
    "    if 'with' in sql_dict:\n",
    "        wit = sql_dict.pop('with')\n",
    "\n",
    "        try:\n",
    "            formatted_query = format(sql_dict)\n",
    "            columns += Parser(formatted_query).columns\n",
    "        except Exception as e:\n",
    "            columns += extract_with(sql_dict)\n",
    "            logging.error(f\"Error parsing query inside extract with: {formatted_query}\",exc_info=True)\n",
    "        if isinstance(wit,list):\n",
    "            for subquery in wit:\n",
    "                subquery_value = subquery['value']\n",
    "                columns += extract_with(subquery_value)\n",
    "        else:\n",
    "            subquery_value = wit['value']\n",
    "            columns += extract_with(subquery_value)\n",
    "    else:\n",
    "        try:\n",
    "            formatted_query = format(sql_dict)\n",
    "            columns += Parser(formatted_query).columns\n",
    "        except Exception as e:\n",
    "            if 'from' in sql_dict:\n",
    "                from_ = sql_dict.pop('from')\n",
    "                sql_dict['from'] = \"placeholder\"\n",
    "                for i in sql_dict.get('select', []):\n",
    "                    if not isinstance(i, dict):\n",
    "                        continue\n",
    "                    value = i.get('value', '')\n",
    "                    if not isinstance(value, dict):\n",
    "                        continue\n",
    "                    if 'select' in value:\n",
    "                        columns += extract_with(i.get('value'))\n",
    "                        i['value'] = 'placeholder'\n",
    "                c += 1\n",
    "                if c > 10:\n",
    "                    print(sql_dict,\"RECURSION\"*10)\n",
    "                    exit()\n",
    "                columns += extract_with(sql_dict,c) ## recursion loop prone\n",
    "                c = 0\n",
    "                if isinstance(from_, list):\n",
    "                    for item in from_:\n",
    "                        columns += extract_with(item)\n",
    "                elif isinstance(from_, dict):\n",
    "                    columns += extract_with(from_)\n",
    "                formatted_query = format(sql_dict)\n",
    "                try:\n",
    "                    columns += Parser(formatted_query).columns\n",
    "                except Exception as e:\n",
    "                    for i in sql_dict.get('select'):\n",
    "                        if 'select' in i.get('value'):\n",
    "                            columns += extract_with(i.get('value'))\n",
    "                            i['value'] = 'placeholder'\n",
    "\n",
    "            else:\n",
    "                if isinstance(sql_dict, list):\n",
    "                    for item in sql_dict:\n",
    "                        columns += self.extract_with(item)\n",
    "    return columns\n",
    " \n",
    " \n",
    "def table(input_sql, report_name):\n",
    "    df_table_temp = pd.DataFrame(Parser(input_sql).tables, columns=['Table'])\n",
    "    df_table_temp['Report_Name'] = report_name\n",
    "    df_table_temp = df_table_temp[['Report_Name', 'Table']]\n",
    "    return df_table_temp\n",
    " \n",
    " \n",
    "def excel_formatting(dict_compare, output_file_name, default_row=50):\n",
    "    cfg = getconfig()\n",
    "    output_file_path = cfg.get('output_path')\n",
    "    file_path = os.path.join(output_file_path, output_file_name)\n",
    "    writer = pd.ExcelWriter(file_path, engine='xlsxwriter')\n",
    " \n",
    "    [dict_compare[i].to_excel(writer, sheet_name=i, index=False) for i in dict_compare.keys()]\n",
    " \n",
    "    workbook = writer.book\n",
    "    sheet_list = list(dict_compare.keys())\n",
    " \n",
    "    [writer.sheets[sheet].set_default_row(default_row) for sheet in sheet_list]\n",
    " \n",
    "    text_format = workbook.add_format({'text_wrap': True,\n",
    "                                       'valign': 'left',\n",
    "                                       'align': 'top',\n",
    "                                       'border': 1\n",
    "                                       })\n",
    " \n",
    "    cell_format = workbook.add_format({'text_wrap': True,\n",
    "                                       'valign': 'left',\n",
    "                                       'align': 'top',\n",
    "                                       'border': 0\n",
    "                                       })\n",
    " \n",
    " \n",
    "    [writer.sheets[sheet].set_column(0, dict_compare[sheet].shape[1]-1,\n",
    "                                     30, text_format) for sheet in sheet_list]\n",
    " \n",
    " \n",
    " \n",
    "    [writer.sheets[sheet].set_row(i,1 , cell_format) for sheet in sheet_list for i in\n",
    "     range(dict_compare[sheet].shape[0]+1, dict_compare[sheet].shape[0]+1000)]\n",
    " \n",
    " \n",
    "    header_format = workbook.add_format(\n",
    "        {'bold': True,\n",
    "         'text_wrap': True,\n",
    "         'valign': 'center',\n",
    "         'fg_color': '#D7E4BC',\n",
    "         'border': 1,\n",
    "         'align': 'top'}\n",
    "    )\n",
    " \n",
    " \n",
    "    [writer.sheets[sheet].write(0, col_num, value, header_format) for sheet in sheet_list\n",
    "     for col_num, value in enumerate(dict_compare[sheet].columns.values)]\n",
    " \n",
    "    writer.close()\n",
    " \n",
    "def excel_formatting_metadata(df_table, df_column, df_join_condition, output_file_path, report_name, default_row=50):\n",
    "    input_file_path = getconfig()['input_path']\n",
    "    output_file_name = 'Metadata_extract' + '_' + datetime.now().strftime('%Y%m%d') + '_' + datetime.now().strftime(\n",
    "        '%H%M') + '.xlsx'\n",
    "\n",
    "    file_path = os.path.join(output_file_path, output_file_name)\n",
    "    print(file_path)\n",
    "    writer = pd.ExcelWriter(file_path, engine='xlsxwriter')\n",
    "    df_table.to_excel(writer, sheet_name='Table List', index=False)\n",
    "    df_column.to_excel(writer, sheet_name='Column_Details', index=False)\n",
    "    df_join_condition.to_excel(writer, sheet_name='Joining_Condition', index=False)\n",
    "    dict_row_column = {}\n",
    "    dict_df = {}\n",
    "    workbook = writer.book\n",
    "    sheet_list = ['Table List',  'Column_Details', 'Joining_Condition'] #+ [i for i in dict_compare.keys()]\n",
    "    dict_row_column['Table List'] = df_table.shape\n",
    "    dict_row_column['Column_Details'] = df_column.shape\n",
    "    dict_row_column['Joining_Condition'] = df_join_condition.shape\n",
    "\n",
    "    dict_df['Table List'] = df_table\n",
    "    dict_df['Column_Details'] = df_column\n",
    "    dict_df['Joining_Condition'] = df_join_condition\n",
    "\n",
    "\n",
    "    [writer.sheets[sheet].set_default_row(default_row) for sheet in sheet_list]\n",
    "\n",
    "    text_format = workbook.add_format({'text_wrap': True,\n",
    "                                       'valign': 'left',\n",
    "                                       'align': 'top',\n",
    "                                       'border': 1\n",
    "                                       })\n",
    "    cell_format = workbook.add_format({'text_wrap': True,\n",
    "                                       'valign': 'left',\n",
    "                                       'align': 'top',\n",
    "                                       'border': 0\n",
    "                                       })\n",
    "\n",
    "    [writer.sheets[sheet].set_column(0, dict_row_column[sheet][1]-1,\n",
    "                                     30, text_format) for sheet in sheet_list]\n",
    "\n",
    "\n",
    "    [writer.sheets[sheet].set_row(i,1 , cell_format) for sheet in sheet_list for i in\n",
    "     range(dict_row_column[sheet][0]+1, dict_row_column[sheet][0]+100)]\n",
    "\n",
    "    header_format = workbook.add_format(\n",
    "        {'bold': True,\n",
    "         'text_wrap': True,\n",
    "         'valign': 'center',\n",
    "         'fg_color': '#D7E4BC',\n",
    "         'border': 1,\n",
    "         'align': 'top'}\n",
    "    )\n",
    "\n",
    "\n",
    "    [writer.sheets[sheet].write(0, col_num, value, header_format) for sheet in sheet_list\n",
    "     for col_num, value in enumerate(dict_df[sheet].columns.values)]\n",
    "\n",
    "    writer.close()\n",
    "\n",
    "\n",
    "def extract_join_info(join_info, dict_table, joining_type, joining_condition, joining_columns_ta, \n",
    "                      joining_columns, lefttable_subquery, righttable_subquery):\n",
    "    \"\"\"Extract join information from a single join.\"\"\"\n",
    "    if isinstance(join_info, str):\n",
    "        dict_table[join_info.lower()] = format(join_info)\n",
    "        return\n",
    "    elif 'pivot' in join_info:\n",
    "        return\n",
    "    elif 'on' not in join_info and 'using' not in join_info:\n",
    "        if 'value' in join_info:\n",
    "            dict_table[join_info['name'].lower()] = format(join_info['value'])\n",
    "        else:\n",
    "            cross_join = list(join_info.keys())[0]\n",
    "            join_info = join_info[cross_join]\n",
    "        return\n",
    "    elif isinstance(join_info[list(join_info.keys())[0]], str):\n",
    "        join_key = list(join_info.keys())[0]\n",
    "        join_value = join_info[join_key]\n",
    "        dict_table[join_value.lower()] = format(join_value)\n",
    "    else:\n",
    "        join_key = list(join_info.keys())[0]\n",
    "        join_value = join_info[join_key]\n",
    "        dict_table[join_value['name'].lower()] = format(join_value['value'])\n",
    "    joining_type.append(join_key)\n",
    "    if 'on' in join_info:\n",
    "        joining_condition.append(list(join_info['on'].keys())[0])\n",
    "    elif 'using' in join_info:\n",
    "        joining_condition.append(list(join_info['using'])[0])\n",
    "    try:\n",
    "        if 'on' in join_info:\n",
    "            joining_columns_ta.append(','.join(list(join_info['on'].values())[0]))\n",
    "            left_part, right_part = list(join_info['on'].values())[0]\n",
    "        elif 'using' in join_info:\n",
    "            joining_columns_ta.append(','.join(join_info['using']))\n",
    "            left_part = join_info['using']\n",
    "            right_part = join_info['using']\n",
    "\n",
    "    except TypeError as e:\n",
    "        def extract_values(data):\n",
    "            result = []\n",
    "            if isinstance(data, dict):\n",
    "                for key, value in data.items():\n",
    "                    if key == 'literal':\n",
    "                        continue\n",
    "                    result.extend(extract_values(value))\n",
    "            elif isinstance(data, list):\n",
    "                for item in data:\n",
    "                    result.extend(extract_values(item))\n",
    "            elif isinstance(data, str):\n",
    "                result.append(data)\n",
    "    \n",
    "            return result\n",
    "        sub_value = extract_values(list(join_info['on'].values()))\n",
    "        joining_columns_ta.append(', '.join(sub_value))  # Join the strings\n",
    "        left_part, right_part = sub_value[:2]\n",
    "    left_part, right_part = left_part.lower(), right_part.lower()\n",
    "    if '.' in left_part:\n",
    "        left_part_tbl, left_part_clm = left_part.split('.')\n",
    "        lefttable_subquery.append(dict_table.get(left_part_tbl, '')[:6].lower() == 'select' and dict_table[left_part_tbl] or '')\n",
    "    else:\n",
    "        left_part_tbl, left_part_clm = '', left_part\n",
    "        lefttable_subquery.append('')\n",
    "\n",
    "    if '.' in right_part:\n",
    "        right_part_tbl, right_part_clm = right_part.split('.')\n",
    "        righttable_subquery.append(dict_table.get(right_part_tbl, '')[:6].lower() == 'select' and dict_table[right_part_tbl] or '')\n",
    "        \n",
    "    else:\n",
    "        right_part_tbl, right_part_clm = '', right_part\n",
    "        righttable_subquery.append('')\n",
    "    if left_part_tbl == '' and right_part_tbl == '':\n",
    "        joining_columns.append(\n",
    "            f\"{left_part_clm},{right_part_clm}\")\n",
    "    elif left_part_tbl == '':\n",
    "        joining_columns.append(\n",
    "            f\"{left_part_clm},{right_part_tbl if dict_table[right_part_tbl].lower().startswith('select') else dict_table[right_part_tbl]}.{right_part_clm}\"\n",
    "        )\n",
    "    elif right_part_tbl == '':\n",
    "        joining_columns.append(\n",
    "            f\"{left_part_tbl if dict_table[left_part_tbl].lower().startswith('select') else dict_table[left_part_tbl]}.{left_part_clm},{right_part_clm}\"\n",
    "        )\n",
    "    else:\n",
    "        joining_columns.append(\n",
    "            f\"{left_part_tbl if dict_table[left_part_tbl].lower().startswith('select') else dict_table[left_part_tbl]}.{left_part_clm},\"\n",
    "            f\"{right_part_tbl if dict_table[right_part_tbl].lower().startswith('select') else dict_table[right_part_tbl]}.{right_part_clm}\"\n",
    "        )\n",
    "    # print(len(joining_type), len(joining_condition), len(joining_columns_ta), len(joining_columns), len(lefttable_subquery), len(righttable_subquery))\n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "    \n",
    "\n",
    "def join_condition(input_sql, report_name):\n",
    "    \"\"\"\n",
    "    Processes SQL input and extracts join conditions into a DataFrame.\n",
    "\n",
    "    :param input_sql: The SQL query to analyze.\n",
    "    :param report_name: The name of the report for identification.\n",
    "    :return: A DataFrame containing join conditions.\n",
    "    \"\"\"\n",
    "    parsed_sql = parse(input_sql)\n",
    "    dict_table = {}\n",
    "    joining_info = {\n",
    "        \"type\": [], \"condition\": [], \"columns_ta\": [],\n",
    "        \"columns\": [], \"left_subquery\": [], \"right_subquery\": []\n",
    "    }\n",
    "\n",
    "    # Initialize joining_columns\n",
    "    joining_columns = []\n",
    "\n",
    "    if parsed_sql.get('from'):\n",
    "        for join in parsed_sql['from']:\n",
    "            extract_join_info(join, dict_table, joining_info[\"type\"], joining_info[\"condition\"],\n",
    "                              joining_info[\"columns_ta\"], joining_columns,\n",
    "                              joining_info[\"left_subquery\"], joining_info[\"right_subquery\"])\n",
    "\n",
    "    else:\n",
    "        # Will do if any case arises like this\n",
    "        # if list(n.keys()) == ['select']:\n",
    "        #     logging.info(f'Skipped following query in join parsing {n}')\n",
    "        #     return pd.DataFrame(columns=['Report_Name', 'Union/Intersection/Minus if any', 'Joining_Type',\n",
    "        #                        'Joining_Condition', 'Joining_Columns_With_Table_Alias_if_any',\n",
    "        #                        'Joining_Columns_Having_Proper_Table_Details', \n",
    "        #                        'LeftTable_Subquery_if_any', 'RightTable_Subquery_if_any'])\n",
    "        try:\n",
    "            for m in parsed_sql.keys():\n",
    "                for index, n in enumerate(parsed_sql[m]):\n",
    "                    if n.get('from'):\n",
    "                        for p in n['from']:\n",
    "                            extract_join_info(p, dict_table, joining_info[\"type\"], joining_info[\"condition\"],\n",
    "                                            joining_info[\"columns_ta\"], joining_columns,\n",
    "                                            joining_info[\"left_subquery\"], joining_info[\"right_subquery\"])\n",
    "                            joining_info[\"columns\"].append(f\"{m}_{index}\")\n",
    "                    else:\n",
    "                        if list(n.keys()) == ['select']:\n",
    "                            logging.info(f'Skipped following query in join parsing {n}')\n",
    "                            continue\n",
    "                        if 'from' not in n.get('value'):\n",
    "                            op = list(n.get('value').keys())[0]\n",
    "                            val = n.get('value').get(op)\n",
    "                            for index, f in enumerate(val):\n",
    "                                for p in f.get('from'):\n",
    "                                    extract_join_info(p, dict_table, joining_info[\"type\"], joining_info[\"condition\"],\n",
    "                                                    joining_info[\"columns_ta\"], joining_columns,\n",
    "                                                    joining_info[\"left_subquery\"], joining_info[\"right_subquery\"])\n",
    "                                    joining_info[\"columns\"].append(f\"{m}_{index}\")\n",
    "                        else:\n",
    "                            for p in n.get('value').get('from'):\n",
    "                                extract_join_info(p, dict_table, joining_info[\"type\"], joining_info[\"condition\"],\n",
    "                                                joining_info[\"columns_ta\"], joining_columns,\n",
    "                                                joining_info[\"left_subquery\"], joining_info[\"right_subquery\"])\n",
    "                                joining_info[\"columns\"].append(f\"{m}_{index}\")\n",
    "\n",
    "        except KeyError as e:\n",
    "            logging.error(e,\"is not found\")\n",
    "        except Exception as e:\n",
    "            logging.error(\"Error processing secondary join conditions\", exc_info=True)\n",
    "    df_join_condition = pd.DataFrame({\n",
    "        'Joining_Type': joining_info[\"type\"],\n",
    "        'Joining_Condition': joining_info[\"condition\"],\n",
    "        'Joining_Columns_With_Table_Alias_if_any': joining_info[\"columns_ta\"],\n",
    "        'Joining_Columns_Having_Proper_Table_Details': joining_columns,\n",
    "        'LeftTable_Subquery_if_any': joining_info[\"left_subquery\"],\n",
    "        'RightTable_Subquery_if_any': joining_info[\"right_subquery\"],\n",
    "    })\n",
    "    \n",
    "    df_join_condition['Report_Name'] = report_name\n",
    "    df_join_condition['Union/Intersection/Minus if any'] = 'NA'\n",
    "    df_join_condition = df_join_condition.drop_duplicates()\n",
    "        \n",
    "    return df_join_condition[['Report_Name', 'Union/Intersection/Minus if any', 'Joining_Type',\n",
    "                               'Joining_Condition', 'Joining_Columns_With_Table_Alias_if_any',\n",
    "                               'Joining_Columns_Having_Proper_Table_Details', \n",
    "                               'LeftTable_Subquery_if_any', 'RightTable_Subquery_if_any']]\n",
    " \n",
    "\n",
    "def calculate_matching_percentages(common_tables, all_tables, common_joins, all_joins):\n",
    "    table_percent_match = (len(common_tables) / len(all_tables)) * 100 if all_tables else 0\n",
    "    join_percent_match = (len(common_joins) / len(all_joins)) * 100 if all_joins else 0\n",
    "    overall_matching_percentage = table_percent_match if len(all_joins) == 0 else (table_percent_match + join_percent_match) / 2\n",
    "    return table_percent_match, join_percent_match, overall_matching_percentage\n",
    "\n",
    "\n",
    "\n",
    "def analyze_reports(overall_df, report_files):\n",
    "    overall_df['Report Combinations'] = overall_df['Report Combinations'].apply(lambda x: x.split(','))\n",
    "\n",
    "    # Extract the matching files from the DataFrame as a list of lists\n",
    "    matching_files = overall_df['Report Combinations'].tolist()\n",
    "    # Mapping of files to reports\n",
    "    file_to_report = {}\n",
    "    for report, files in report_files.items():\n",
    "        for file in files:\n",
    "            file_to_report[file] = report\n",
    "    \n",
    "    # Analyze full report merging\n",
    "    report_merge_candidates = defaultdict(set)\n",
    "    file_merge_map = defaultdict(set)\n",
    "\n",
    "    for match_group in matching_files:\n",
    "        # For each group of matching files, associate all reports\n",
    "        reports_in_group = set(file_to_report[file] for file in match_group)\n",
    "        for file in match_group:\n",
    "            report = file_to_report[file]\n",
    "            report_merge_candidates[report].update(reports_in_group - {report})\n",
    "            file_merge_map[report].update(match_group)\n",
    "    \n",
    "    fully_mergeable_reports = set()\n",
    "    partially_mergeable_reports = defaultdict(list)\n",
    "    within_report_consolidations = defaultdict(list)\n",
    "\n",
    "    for report, files in report_files.items():\n",
    "        matched_files = set(file_merge_map[report])\n",
    "        if matched_files == set(files):\n",
    "            # If all files in the report are matched, the report can be fully merged\n",
    "            fully_mergeable_reports.add(report)\n",
    "        else:\n",
    "            # For partial merging, calculate how many files can be merged with others\n",
    "            partially_merged_files = matched_files.intersection(files)\n",
    "            if partially_merged_files:\n",
    "                partially_mergeable_reports[report] = {\n",
    "                    \"total_files\": len(files),\n",
    "                    \"matched_files\": len(partially_merged_files),\n",
    "                    \"reports_matched_with\": report_merge_candidates[report]\n",
    "                }\n",
    "        # Intra-report file consolidation analysis\n",
    "        within_report_matches = [file for file in files if any(other_file in file_merge_map[report] and other_file != file for other_file in files)]\n",
    "        if within_report_matches:\n",
    "            within_report_consolidations[report] = within_report_matches\n",
    "    \n",
    "    # Output\n",
    "    print(\"Reports that can be fully merged and removed:\")\n",
    "    print(fully_mergeable_reports)\n",
    "\n",
    "    print(\"\\nReports that can be partially merged:\")\n",
    "    for report, merge_info in partially_mergeable_reports.items():\n",
    "        print(f\"Report: {report}\")\n",
    "        print(f\"  Total files: {merge_info['total_files']}\")\n",
    "        print(f\"  Matched files: {merge_info['matched_files']}\")\n",
    "        print(f\"  Reports matched with: {merge_info['reports_matched_with']}\")\n",
    "\n",
    "    print(\"\\nWithin-report file consolidations (files that can be consolidated):\")\n",
    "    for report, consolidations in within_report_consolidations.items():\n",
    "        print(f\"Report: {report}\")\n",
    "        print(f\"  Files that can be consolidated: {consolidations}\")\n",
    "\n",
    "\n",
    "def compare(list1, df_table, df_joining):\n",
    "    dict_compare = {}\n",
    "    not_recommended_combinations = set()\n",
    "    for i in range(2, len(list1) + 1): # loop 1\n",
    "        sheet_name = f'Comparison_{i}'\n",
    "        df_temp = pd.DataFrame(columns=[\n",
    "            'Report Combinations', '# of tables across all reports', \n",
    "            '# of joining keys across all reports', '# of common tables',\n",
    "            '# of common joining keys', 'Table Matching Percentage',\n",
    "            'Joining Keys Matching Percentage', 'Overall Matching Percentage', \n",
    "            'Recommendation'\n",
    "        ])\n",
    "\n",
    "        break_loop = True\n",
    "        for combo in combinations(list1, i): # loop 2\n",
    "            combo_set = frozenset(combo)  \n",
    "            \n",
    "            if any(nrc.issubset(combo_set) for nrc in not_recommended_combinations):\n",
    "                recommendation = 'Reports are having different sets of data, should not get merged'\n",
    "                df1 = pd.DataFrame({\n",
    "                    'Report Combinations': ', '.join(combo),\n",
    "                    '# of tables across all reports': 'N/A',\n",
    "                    '# of joining keys across all reports': 'N/A',\n",
    "                    '# of common tables': 'N/A',\n",
    "                    '# of common joining keys': 'N/A',\n",
    "                    'Table Matching Percentage': 'N/A',\n",
    "                    'Joining Keys Matching Percentage': 'N/A',\n",
    "                    'Overall Matching Percentage': 'N/A',\n",
    "                    'Recommendation': recommendation\n",
    "                }, index=[0])\n",
    "                # df_temp = pd.concat([df_temp, df1], ignore_index=True)\n",
    "                continue \n",
    "            all_tables = set(df_table[df_table['Report_Name'].isin(combo)]['Table'])\n",
    "            all_joins = set(df_joining[df_joining['Report_Name'].isin(combo)]['Joining_Columns_Having_Proper_Table_Details'])\n",
    "            \n",
    "            dict_set = {report: set(df_table[df_table['Report_Name'] == report]['Table']) for report in combo}\n",
    "            dict_join = {report: set(df_joining[df_joining['Report_Name'] == report]['Joining_Columns_Having_Proper_Table_Details']) for report in combo}\n",
    "\n",
    "            common_tables = set.intersection(*(dict_set.values()))\n",
    "            common_joins = set.intersection(*(dict_join.values()))\n",
    "\n",
    "            table_percent_match, join_percent_match, overall_matching_percentage = calculate_matching_percentages(\n",
    "                common_tables, all_tables, common_joins, all_joins\n",
    "            )\n",
    "            \n",
    "            recommendation = 'Reports can be merged' if overall_matching_percentage > 80 else 'Reports are having different sets of data, should not get merged'\n",
    "\n",
    "            if overall_matching_percentage < 80:\n",
    "                not_recommended_combinations.add(combo_set)\n",
    "            else:\n",
    "                break_loop = False\n",
    "\n",
    "            df1 = pd.DataFrame({\n",
    "                'Report Combinations': ','.join(combo),\n",
    "                '# of tables across all reports': len(all_tables),\n",
    "                '# of joining keys across all reports': len(all_joins),\n",
    "                '# of common tables': len(common_tables),\n",
    "                '# of common joining keys': len(common_joins),\n",
    "                'Table Matching Percentage': round(table_percent_match, 2),\n",
    "                'Joining Keys Matching Percentage': round(join_percent_match, 2),\n",
    "                'Overall Matching Percentage': round(overall_matching_percentage, 2),\n",
    "                'Recommendation': recommendation\n",
    "            }, index=[0])\n",
    "            if df_temp.empty:\n",
    "                df_temp = df1\n",
    "            else:\n",
    "                df_temp = pd.concat([df_temp, df1], ignore_index=True)\n",
    "\n",
    "        if break_loop and sheet_name != 'Comparison_2':\n",
    "            break\n",
    "        dict_compare[sheet_name] = df_temp\n",
    "    return dict_compare\n",
    "\n",
    "def column_details(input_sql, report_name):\n",
    "    try:\n",
    "        cols = Parser(input_sql).columns\n",
    "    except Exception as e:\n",
    "        try:\n",
    "            parsed = parse(input_sql)\n",
    "            cols = extract_with(parsed)\n",
    "        except Exception as ex:\n",
    "            logging.error(f\"Error parsing query in column details: {input_sql}\",exc_info=True)\n",
    "            tabs = Parser(input_sql).tables\n",
    "            cols = [i+'.' for i in tabs]\n",
    "    list1 = [i.split('.') for i in cols]\n",
    "    if list1 == []:\n",
    "        return pd.DataFrame(columns=['Report_Name', 'Db', 'Schema', 'Table', 'Column'])\n",
    "    for i in list1:\n",
    "        if len(i) == 3:\n",
    "            i.insert(0, '')\n",
    "    list3 = [['', ''] + i if len(i) == 2 else  ['', '', ''] + i  if len(i)==1 else i for i in list1]\n",
    "    #list3 = [['', '', ''] + i if len(i) == 1 else i for i in list1]\n",
    "    if len(Parser(input_sql).tables)>0:\n",
    "        table_var = Parser(input_sql).tables[0].split('.')\n",
    "    else:\n",
    "        table_var = []\n",
    "    if len(table_var) == 3:\n",
    "        db, schema, table = table_var\n",
    "    elif len(table_var) == 2:\n",
    "        db = ''\n",
    "        schema, table = table_var\n",
    "    elif len(table_var) == 1:\n",
    "        db = ''\n",
    "        schema = ''\n",
    "        table = table_var[0]\n",
    "    else:\n",
    "        db, schema, table = '','',''\n",
    "\n",
    "    if (len(Parser(input_sql).tables) == 1) & (max([len(i) for i in list3]) == 1):\n",
    "        list4 = [[db, schema, table] + i for i in list3]\n",
    "        df_column_temp = (\n",
    "            pd.DataFrame(list4)\n",
    "            .dropna()\n",
    "            .apply(lambda col: col.map(lambda x: x.lower() if isinstance(x, str) else x))\n",
    "            .drop_duplicates()\n",
    "        )\n",
    "        df_column_temp = df_column_temp.iloc[:, -4:]\n",
    "        df_column_temp.columns = ['Db', 'Schema', 'Table', 'Column']\n",
    "        df_column_temp['Report_Name'] = report_name\n",
    "        df_column_temp = df_column_temp[['Report_Name', 'Db', 'Schema', 'Table', 'Column']]\n",
    "    else:\n",
    "        df_column_temp = (\n",
    "            pd.DataFrame(list3)\n",
    "            .dropna()\n",
    "            .apply(lambda col: col.map(lambda x: x.lower() if isinstance(x, str) else x))\n",
    "            .drop_duplicates()\n",
    "        )\n",
    "        df_column_temp = df_column_temp.iloc[:, -4:]\n",
    "        df_column_temp.columns = ['Db', 'Schema', 'Table', 'Column']\n",
    "        df_column_temp['Report_Name'] = report_name\n",
    "        df_column_temp = df_column_temp[['Report_Name', 'Db', 'Schema', 'Table', 'Column']]\n",
    "    \n",
    "    # Identify rows where 'Table' is not empty\n",
    "    # non_empty_table = df_column_temp[df_column_temp['Table'] != '']\n",
    "\n",
    "    # # Identify rows where 'Table' is empty and there exists a matching (Report_Name, Column)\n",
    "    # remove_rows = df_column_temp[\n",
    "    #     (df_column_temp['Table'] == '') &\n",
    "    #     df_column_temp[['Report_Name', 'Column']].apply(tuple, axis=1).isin(\n",
    "    #         non_empty_table[['Report_Name', 'Column']].apply(tuple, axis=1)\n",
    "    #     )\n",
    "    # ]\n",
    "    # # Remove the identified rows\n",
    "    # df_column_temp = df_column_temp.drop(remove_rows.index)\n",
    "    return df_column_temp\n",
    "\n",
    "def remove_subset_files(df):\n",
    "    df['set_values'] = df['Report Combinations'].apply(lambda x: set(x.split(',')))\n",
    "\n",
    "    # Remove duplicates and subsets\n",
    "    df = df.drop_duplicates(subset='set_values', keep='first')  # Remove exact duplicates\n",
    "    df_cleaned = df[~df['set_values'].apply(lambda x: any(x < y for y in df['set_values']))]\n",
    "\n",
    "    # Output the cleaned DataFrame\n",
    "    df_cleaned = df_cleaned.drop(columns=['set_values']).reset_index(drop=True)\n",
    "\n",
    "    return df_cleaned\n",
    "\n",
    "\n",
    "def create_sql_files(twb_path: str, output_dir: str = \"sql_for_comparison\") -> None:\n",
    "    \"\"\"\n",
    "    Extracts SQL statements from Tableau workbook XML files and saves them in individual SQL files.\n",
    "\n",
    "    Args:\n",
    "        path (List[str]): List of file paths to Tableau workbook XML files.\n",
    "        output_dir (str): Directory where SQL files will be saved. Defaults to \"sql_for_comparison\".\n",
    "\n",
    "    \"\"\"\n",
    "    # Ensure the output directory exists\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    twb_files = os.listdir(twb_path)\n",
    "    for twb_file in twb_files:\n",
    "        full_path = os.path.join(twb_path, twb_file)\n",
    "        tree = ET.parse(full_path) \n",
    "        root = tree.getroot()\n",
    "        sql = root.findall(\".//relation\")\n",
    "        unique_sql = {elem.text.strip() for elem in sql if elem.text}\n",
    "        \n",
    "        for index, datasource in enumerate(unique_sql):\n",
    "            if datasource:\n",
    "                datasource_text = datasource.strip().replace('//', '--')\\\n",
    "                    .replace('<<', '<').replace('>>', '>').replace('::', ':')\n",
    "                filename = f\"{twb_file.removesuffix('.twb')}-sql{index}.sql\"\n",
    "                filepath = os.path.join(output_dir, filename)\n",
    "                \n",
    "                # Write `datasource_text` to the file\n",
    "                with open(filepath, 'w') as file:\n",
    "                    file.write(datasource_text)\n",
    "    return output_dir\n",
    "\n",
    "def remove_sql_files(output_dir: str) -> None:\n",
    "    \"\"\"\n",
    "    Removes all SQL files in the specified directory.\n",
    "\n",
    "    Args:\n",
    "        output_dir (str): Directory containing SQL files to remove.\n",
    "\n",
    "    \"\"\"\n",
    "    if os.path.exists(output_dir):\n",
    "        for file in os.listdir(output_dir):\n",
    "            os.remove(os.path.join(output_dir, file))\n",
    "\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    sql_for_comparison = 'sql_for_comparison'\n",
    "    remove_sql_files(sql_for_comparison)\n",
    "    input_file_path = getconfig()['input_path']\n",
    "    sql_file_path = create_sql_files(input_file_path, sql_for_comparison)\n",
    "    files = os.listdir(sql_file_path) \n",
    "    report = {}\n",
    "\n",
    "    for i in files:\n",
    "        before_dash = i.split('-')[0].strip()\n",
    "        if before_dash not in report:\n",
    "            report[before_dash] = []\n",
    "        report[before_dash].append(i)\n",
    "        print(i)\n",
    "        report_name = i\n",
    "        with open(os.path.join(sql_file_path, i), 'r') as file:\n",
    "            input_sql = file.read()\n",
    "        # input_sql = input_sql.replace('\"', '')\n",
    "        input_sql = input_sql.replace('[', ' ').replace(']',' ') \n",
    "        df_column_temp = column_details(input_sql, report_name)\n",
    "        input_sql = input_sql.replace('[', '')\n",
    "        input_sql = input_sql.replace(']', '')\n",
    "        df_table_temp = table(input_sql, report_name)\n",
    "        df_join_condition_temp = join_condition(input_sql, report_name)\n",
    "        df_table = pd.concat([df_table, df_table_temp])\n",
    "        df_join_condition = pd.concat([df_join_condition, df_join_condition_temp])\n",
    "        df_column = pd.concat([df_column,df_column_temp])\n",
    "\n",
    "    \n",
    "    # only when we have completed parsed output\n",
    "    # df_column = pd.read_excel('SQL Report Analysis-Consolidation with deletion 23-10.xlsx')\n",
    "\n",
    "    \n",
    "    df_column['Table'] = df_column['Table'].fillna('').str.lower()\n",
    "    lookup['table'] = lookup['Name'].fillna('').str.lower()\n",
    "    df_column['tab'] = df_column['Table'].str.split('.').str[-1]\n",
    "    merged_df = df_column.merge(lookup, left_on='tab', right_on='table', how='left', suffixes=('', '_lookup'))\n",
    "    merged_df['Db'] = merged_df['Db'].replace('',pd.NA).fillna(merged_df['db']).str.lower()\n",
    "    merged_df['Db'] = merged_df['Db'].str.lower()\n",
    "    merged_df.drop(columns=['Name', 'Context','tab','table', 'loc', 'db', 'schema'], inplace=True)\n",
    "    merged_df.drop_duplicates(inplace=True)\n",
    "    excel_formatting_metadata(df_table, merged_df, df_join_condition, '', 'Metadata_extract')\n",
    "    db_dict = {}\n",
    "    for index, row in merged_df.iterrows():\n",
    "        sql_file = row.iloc[0]\n",
    "        database = row.iloc[1]\n",
    "        table_ = row.iloc[3] \n",
    "        \n",
    "        if pd.isna(database) or database == '':\n",
    "            continue\n",
    "        \n",
    "        if database not in db_dict:\n",
    "            db_dict[database] = {}\n",
    "        \n",
    "        if table_ not in db_dict[database]:\n",
    "            db_dict[database][table_] = set()\n",
    "        \n",
    "        db_dict[database][table_].add(sql_file)\n",
    "\n",
    "    df_temp = pd.DataFrame(columns=[\n",
    "            'Report Combinations', '# of tables across all reports', \n",
    "            '# of joining keys across all reports', '# of common tables',\n",
    "            '# of common joining keys', 'Table Matching Percentage',\n",
    "            'Joining Keys Matching Percentage', 'Overall Matching Percentage', \n",
    "            'Recommendation'\n",
    "        ])\n",
    "    \n",
    "    for db in db_dict:\n",
    "        for table_ in db_dict[db]:\n",
    "            output_file_name = f\"{db}_{table_}\" + '_' + datetime.now().strftime('%Y%m%d') + '_' + datetime.now().strftime('%H%M') + '.xlsx'\n",
    "            combos = list(db_dict[db][table_])\n",
    "            if 20 > len(combos) > 1:\n",
    "                dict_compare = compare(combos, df_table, df_join_condition)\n",
    "                for sheet_name in dict_compare.keys():\n",
    "                    sheet = dict_compare[sheet_name]\n",
    "                    df1 = sheet.loc[sheet['Overall Matching Percentage']>=80,:]\n",
    "                    if df_temp.empty:\n",
    "                        df_temp = df1\n",
    "                    else:\n",
    "                        df_temp = pd.concat([df_temp, df1], ignore_index=True)\n",
    "                # excel_formatting(dict_compare, output_file_name)\n",
    "                \n",
    "            else:\n",
    "                # print(f\"{db}_{table_}\")\n",
    "                pass\n",
    "    di = {}\n",
    "    if not df_temp.empty:\n",
    "        df_temp.drop_duplicates(inplace=True)     \n",
    "        df_temp = remove_subset_files(df_temp)\n",
    "        df_temp['Total files'] = df_temp['Report Combinations'].apply(lambda x: len(x.split(',')))\n",
    "        col = df_temp.pop('Total files')  # Remove the column\n",
    "        df_temp.insert(1, 'Total files', col)\n",
    "        di['Overall'] = df_temp\n",
    "        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "        output_file_name = f\"Overall_{timestamp}.xlsx\"\n",
    "        excel_formatting(di, output_file_name)\n",
    "        print(\"Total number of files :\",len(files))\n",
    "        print(df_temp)\n",
    "        print(f\"{df_temp['Total files'].sum()} files can be consolidated to {df_temp.shape[0]}\")\n",
    "        # Run the analysis\n",
    "        analyze_reports(df_temp, report)\n",
    "    else:\n",
    "        print(\"No Reports can be consolidated\")\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ce634b93-39c2-4c2a-abfb-d2ecbb6d8a33",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting xlsxwriter\n",
      "  Downloading XlsxWriter-3.2.0-py3-none-any.whl.metadata (2.6 kB)\n",
      "Downloading XlsxWriter-3.2.0-py3-none-any.whl (159 kB)\n",
      "Installing collected packages: xlsxwriter\n",
      "Successfully installed xlsxwriter-3.2.0\n"
     ]
    }
   ],
   "source": [
    "!pip install xlsxwriter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "db5f2faa-f7d7-4dcb-bb3d-b5b51f14e7f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting sql_metadata\n",
      "  Downloading sql_metadata-2.15.0-py3-none-any.whl.metadata (9.8 kB)\n",
      "Collecting sqlparse<0.6.0,>=0.4.1 (from sql_metadata)\n",
      "  Downloading sqlparse-0.5.3-py3-none-any.whl.metadata (3.9 kB)\n",
      "Downloading sql_metadata-2.15.0-py3-none-any.whl (22 kB)\n",
      "Downloading sqlparse-0.5.3-py3-none-any.whl (44 kB)\n",
      "Installing collected packages: sqlparse, sql_metadata\n",
      "Successfully installed sql_metadata-2.15.0 sqlparse-0.5.3\n"
     ]
    }
   ],
   "source": [
    "!pip install sql_metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0384a9f6-bdcb-46a7-ba03-079740ee9094",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting mo_sql_parsing\n",
      "  Downloading mo_sql_parsing-11.658.24326-py3-none-any.whl.metadata (10 kB)\n",
      "Collecting mo-dots==10.647.24166 (from mo_sql_parsing)\n",
      "  Downloading mo_dots-10.647.24166-py3-none-any.whl.metadata (4.9 kB)\n",
      "Collecting mo-future==7.584.24095 (from mo_sql_parsing)\n",
      "  Downloading mo_future-7.584.24095-py3-none-any.whl.metadata (2.0 kB)\n",
      "Collecting mo-imports==7.584.24095 (from mo_sql_parsing)\n",
      "  Downloading mo_imports-7.584.24095-py3-none-any.whl.metadata (6.1 kB)\n",
      "Collecting mo-parsing==8.654.24251 (from mo_sql_parsing)\n",
      "  Downloading mo_parsing-8.654.24251-py3-none-any.whl.metadata (7.8 kB)\n",
      "Downloading mo_sql_parsing-11.658.24326-py3-none-any.whl (44 kB)\n",
      "Downloading mo_dots-10.647.24166-py3-none-any.whl (27 kB)\n",
      "Downloading mo_future-7.584.24095-py3-none-any.whl (10 kB)\n",
      "Downloading mo_imports-7.584.24095-py3-none-any.whl (11 kB)\n",
      "Downloading mo_parsing-8.654.24251-py3-none-any.whl (62 kB)\n",
      "Installing collected packages: mo-future, mo-imports, mo-dots, mo-parsing, mo_sql_parsing\n",
      "Successfully installed mo-dots-10.647.24166 mo-future-7.584.24095 mo-imports-7.584.24095 mo-parsing-8.654.24251 mo_sql_parsing-11.658.24326\n"
     ]
    }
   ],
   "source": [
    "!pip install mo_sql_parsing"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
